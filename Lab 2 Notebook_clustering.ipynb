{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import sklearn\n",
    "from sklearn import cluster, datasets, mixture, metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_blobs, make_circles, make_moons\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Clustering\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "## a. K-means\n",
    "\n",
    "------------------\n",
    "\n",
    "### KMeans(n_clusters, init, n_init, max_iter, precompute_distances, algorithm)\n",
    "*  Return an K-means clustering object.\n",
    " \n",
    "* #### Arguments:\n",
    " 1.    n_clusters: number of clusters/number of centroids.Default value: 8\n",
    " 2.    init: method for initialization of clusters. \n",
    "   * \"random\": choose k observations at random from data;\n",
    "   * \"k-means++\": \"smart\" cluster initialization, speed up convergence\n",
    " 3.    n_init: number of times algorithm runs with different centroid seeds. Default value: 10\n",
    " 4.     max_iter: number of iterations for a single run. Default value: 300\n",
    " 5.  precompute_distances: whether to precompute the distances (faster but more memory)\n",
    "\n",
    "*  #### Attributes (of the K-means clustering object):\n",
    " *    cluster_centers_: array [n_clusters, n_featers]\n",
    " *    labels_: labels of each point\n",
    " *    inertia: sum of squared distances of samples to their closest cluster center\n",
    " *    n_iter : number of iterations run\n",
    "\n",
    "*  #### Methods (on the K-means clustering object)\n",
    " *    fit(X) : compute K-means clustering for dataset X\n",
    " *    fit_predict(X): compute K-means clustering and return cluster indices for samples in X\n",
    " *    predict(X): return the index of the closest cluster each sample in X belongs to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercises:\n",
    "## Data\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [10, 2], [10, 4], [10, 0]])\n",
    "\n",
    "# Compute K-means clustering\n",
    "### YOUR CODE HERE (Fill in the \"None\")\n",
    "# Hint: k-means is a K-means clustering object of 2 clusters, random initialization of clusters, 50 iterations\n",
    "kmeans = KMeans(None)\n",
    "\n",
    "# compute K-means clustering for dataset X\n",
    "kmeans = kmeans.fit(X)\n",
    "\n",
    "# Print out the labels of samples in X\n",
    "# Hint: which attribute of the K-means object should we use?\n",
    "print(None)\n",
    "### END OF YOUR CODE\n",
    "\n",
    "# Plot the cluster centers\n",
    "# Hint: which attribute of the K-means object should we use?\n",
    "centers = None\n",
    "plt.plot(centers[:, 0], centers[:, 1], 'ro')\n",
    "\n",
    "# Predict label of the point [8, 3]\n",
    "# Hint: which function should we use? \n",
    "# Hint :when filling argument of the function, set of samples should be a matrix, each sample is a row\n",
    "print(None)\n",
    "plt.plot(8, 3, 'go')\n",
    "### END OF YOUR CODE\n",
    "\n",
    "# Visualization\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex 1.a.2\n",
    "## Data\n",
    "n_samples = 1500\n",
    "random_state = 170\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "# Compute K-means clustering\n",
    "### YOUR CODE HERE (Fill in the \"None\")\n",
    "# Hint: define a K-means object. Guess the appropriate number of clusters.\n",
    "#None\n",
    "# Hint: compute K-means clustering. Variable \"y_pred\" should be cluster indices of samples\n",
    "y_pred = None\n",
    "### END OF YOUR CODE\n",
    "\n",
    "## Visualization\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clustering (cont)\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "## b. Agglomerative Clustering\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "### AgglomerativeClustering (n_clusters, affinity, linkage)\n",
    "*  Return an Agglomerative Clustering object.\n",
    "*  Recursively merge the pair of clusters that minimally increases a given linkage distance.\n",
    "* #### Arguments:\n",
    " 1.    n_clusters: number of clusters/number of centroids. Default value: 2\n",
    " 2,     affinity: metric used to compute the linkage (\"euclidean\", \"l1\", \"l2\", \"manhattan\", \"cosine\",....)\n",
    " 3.    linkage: determines which distance to use between sets of observation. The algorithm merges pairs of cluster minimizing this criterion.\n",
    "  *              \"ward\":    : variance of the clusters being merged, only works with affinity \"euclidean\"\n",
    "  *             \"average\"  : average of the distances of each observation of the 2 sets\n",
    "  *            \"complete\" : maximum distances between all observations of the 2 sets \n",
    "  *             \"single\"   : minimum distances between all observations of the 2 sets\n",
    "* #### Attributes (of the Agglomerative clustering object):\n",
    " *    n_clusters_: array [n_clusters, n_featers]\n",
    " *    labels_: labels of each point\n",
    " *    n_leaves: number of leaves in the hierarchical tree\n",
    "* #### Methods (on the Agglomerative clustering object)\n",
    " *    fit(X) : compute Agglomerative clustering for dataset X and return an Agglomerative Clusting object with attributes\n",
    " *    fit_predict(X): compute Agglomerative clustering for dataset X and return cluster indices for samples in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISES\n",
    "# Ex.1.b.1: Choosing the number of clusters in Agglomerative clustering\n",
    "# Please make sure that you have \"shopping-data.csv\" stored in the same folder as this notebook.\n",
    "# This file contains shopping data of customers. Suppose our task is to segment customers based on their shopping patterns.\n",
    "customer_data = pd.read_csv('shopping-data.csv')  \n",
    "\n",
    "# Before we start, let's explore more about this dataset\n",
    "\n",
    "## Shape of the dataset\n",
    "print(customer_data.shape)\n",
    "\n",
    "## Print the first 5 data items\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.1.b.1: Choosing the number of clusters in Agglomerative clustering (cont)\n",
    "# We suspect that the last two entries could be used for clustering\n",
    "# Extract the last 2 columns\n",
    "data = customer_data.iloc[:, 3:5].values  \n",
    "\n",
    "# Use dendrogram to visualize hierarchical clustering for this dataset\n",
    "plt.figure(figsize=(10, 7))  \n",
    "plt.title(\"Customer Dendograms\")  \n",
    "dend = shc.dendrogram(shc.linkage(data, method='ward'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.1.b.1: Choosing the number of clusters in Agglomerative clustering (cont)\n",
    "\n",
    "# Now, let's make use of the Dendrogram to sucessfully apply Agglomerative clustering \n",
    "# QUESTION: Based on the dendrogram above, what would be the appropriate number of clusters? \n",
    "\n",
    "# Computer Agglomerative Clustering\n",
    "### YOUR CODE HERE (Fill in the \"None\")\n",
    "# Hint: define an Agglomerative Clustering object\n",
    "cluster = AgglomerativeClustering(None)\n",
    "# Hint: compute Agglomerative Clustering for our dataset. The \"cluster\" variable must have attributes.\n",
    "None\n",
    "### END OF YOUR CODE\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 7))  \n",
    "plt.scatter(data[:,0], data[:,1], c=cluster.labels_, cmap='rainbow')  \n",
    "\n",
    "# QUESTION: Can you try with different number of clusters and visually compare the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ex.1.b.2: Visualiation of effects of linkage on clustering result\n",
    "\n",
    "# Number of samples in each sample set\n",
    "n_samples = 1500\n",
    "\n",
    "# Create datasets of particular shapes\n",
    "noisy_circles = make_circles(n_samples=n_samples, factor=.5,noise=.05)    # circles\n",
    "noisy_moons = make_moons(n_samples=n_samples, noise=.05)                  # moons\n",
    "blobs = make_blobs(n_samples=n_samples, random_state=8)                   # blobs\n",
    "no_structure = np.random.rand(n_samples, 2), None                         # no structure\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)                                                      # anisotropic\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = make_blobs(n_samples=n_samples,                                  # blobs with varied variances\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "\n",
    "### IMPORTANT: these give insights about our datasets\n",
    "default_base = {'n_neighbors': 10,\n",
    "                'n_clusters': 3}\n",
    "\n",
    "SampleSets = [\n",
    "    (noisy_circles, {'n_clusters': 2}),\n",
    "    (noisy_moons, {'n_clusters': 2}),\n",
    "    (varied, {'n_neighbors': 2}),\n",
    "    (aniso, {'n_neighbors': 2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "### \n",
    "\n",
    "# Set up cluster parameters\n",
    "plt.figure(figsize=(9 * 1.3 + 2, 14.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "\n",
    "for i_dataset, (sampleset, algo_params) in enumerate(SampleSets):\n",
    "    \n",
    "    X, y = sampleset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    \n",
    "    # update parameters with dataset-specific values\n",
    "    # Recall IMPORTANT section, each dataset has its own structure in terms of number of clusters\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "    \n",
    "    # Define Agglomerative Clustering objects\n",
    "    ### YOUR CODE HERE (fill in the \"None\")\n",
    "    # Hint: How to set the AgglomerativeClustering object adaptive to structure of each particular dataset?\n",
    "    # Hint: changes of affinity do not affect the result in most cases\n",
    "    # Hint: the variable name suggests the type of Agglomerative Clustering object we are defining\n",
    "    ward = None \n",
    "    average = None\n",
    "    complete = None\n",
    "    single = None\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('Ward Linkage', ward),\n",
    "        ('Average Linkage', average),\n",
    "        ('Complete Linkage', complete),\n",
    "        ('Single Linkage', single)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        \n",
    "        # Prediction\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(SampleSets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "        \n",
    "        # Visualization\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clustering (cont)\n",
    "----------------------\n",
    "## c. Density-based spatial clustering of applications with noise (DBSCAN)\n",
    "----------------------\n",
    "\n",
    "### DBSCAN(eps, min_samples, metric, leaf_size)\n",
    "* Return an DBSCAN clustering object.\n",
    "* Find core samples of high density and expand clusters from them. Good for data which contains clusters of similar density.\n",
    "* #### Arguments:\n",
    " 1.     eps: maximum distance between 2 samples for one to be considered as in the neighborhood of the other. NOT maximum bound on the distances of points within a cluster.\n",
    " 2.    min_samples: the number of samples in a neighborhood for a point to be considered as a core point. Inclues the point itself. controls the level of tolerance of the algorithms towards noise.\n",
    " 3.    metric: metric used to calculated the distance (\"euclidean\", \"cosine\", \"l1\", \"l2\", \"manhattan\",....)\n",
    "\n",
    "* ##### Attributes (of the DBSCAN clustering object):\n",
    " *    core_sample_indices: indices of core samples\n",
    " *    components: copy of each core sample found by training\n",
    " *    labels_: cluster labels for each point in the dataset given to fit(). Noises labeled (-1)\n",
    "\n",
    "* #### Methods (on the DBSCAN clustering object)\n",
    " *    fit(X) : compute DBSCAN clustering for dataset X and return a DBSCAN clustering object with attributes\n",
    " *    fit_predict(X): compute clustering for dataset X and return cluster labels (as an array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Ex.1.c.1: DBSCAN practice\n",
    "\n",
    "# Generate sample data\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n",
    "                            random_state=0)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "\n",
    "# Compute DBSCAN clustering\n",
    "### YOUR CODE HERE (Fill in the \"None\")\n",
    "# Hint: do grid-search on [0,0.5] to find good \"eps\"; try several \"min_samples\" to see the effect on the black points (noises)\n",
    "dbscan = None\n",
    "# Hint: db has attributes\n",
    "db = None\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "# Hint: variable \"labels\" should contain the labels of our samples\n",
    "labels = None\n",
    "### END OF YOUR CODE\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "\n",
    "\n",
    "# Plot result\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.1.c.2: DBSCAN vs. K-means vs. Agglomerative\n",
    "# One issue with K-means clustering and Agglomerative is that it assumes all directions are equally important.\n",
    "# Unfortunately, directions are not always equally important. This exercise explores 1 such example.\n",
    "\n",
    "# generate some random cluster data\n",
    "X, y = make_blobs(random_state=170, n_samples=600, centers = 5)\n",
    "rng = np.random.RandomState(74)\n",
    "# transform the data to be stretched\n",
    "transformation = rng.normal(size=(2, 2))\n",
    "X = np.dot(X, transformation)\n",
    "# plot\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.1.c.2: DBSCAN vs. k-means vs. Agglomerative (cont)\n",
    "# As you can see, we have clusters with a stretched diagonal shape.\n",
    "# QUESTION: what would be the appropriate number of clusters?\n",
    "# Answer: 5 would be a reasonable choice.\n",
    "\n",
    "# Let's see how well K-means and Agglomerative clustering techniques work in this case\n",
    "\n",
    "### Compute K-means clustering for dataset X\n",
    "### YOUR CODE HERE (Fill in the \"None\"). 2 lines of code\n",
    "# Hint: make sure that variable \"y_pred\" is the array of cluster indices\n",
    "None\n",
    "None\n",
    "### END OF YOUR CODE.\n",
    "\n",
    "# plot the cluster assignments and cluster centers\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=\"plasma\")\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0],   \n",
    "            kmeans.cluster_centers_[:, 1],\n",
    "            marker='^', \n",
    "            c=[0, 1, 2, 3, 4], \n",
    "            s=100, \n",
    "            linewidth=2,\n",
    "            cmap=\"plasma\")\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.1.c.2: DBSCAN vs. k-means vs. Agglomerative (cont)\n",
    "\n",
    "### Compute Agglomerative clustering for dataset X\n",
    "### YOUR CODE HERE (Fill in the \"None\"). 2 lines of code\n",
    "None\n",
    "None\n",
    "### END OF YOUR CODE.\n",
    "\n",
    "# plot the cluster assignments and cluster centers\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=\"plasma\")\n",
    "\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.1.c.2: DBSCAN vs. k-means vs. Agglomerative (cont)\n",
    "\n",
    "### Standardize features by removing the mean and scaling to unit variance\n",
    "X = StandardScaler().fit_transform(X)\n",
    "# Attendees may question that this is not a fair judgement since only DBSCAN has this line. If so, ask them to add this line \n",
    "# before computing K-means or Agglomerative clustering, it would not improve the performance :)\n",
    "\n",
    "### YOUR CODE HERE (Fill in the \"None\"). 2 lines of code\n",
    "None\n",
    "None\n",
    "### END OF YOUR CODE\n",
    "\n",
    "# plot the cluster assignments\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=\"plasma\")\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "\n",
    "# QUESTION: What do you see from the results of 3 clustering techiques on this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clustering (cont) - OPTIONAL (if time permits)\n",
    "------------------------\n",
    "## d. Expectation Maximization (on Gaussian Mixture Model)\n",
    "------------------------\n",
    " Can be understood as a probabilistic extention of K-means clustering \n",
    " i.e., each sample has a probability distribution over possible labels\n",
    "\n",
    "### GaussianMixture(n_components, covariance_type, max_iter)\n",
    "* Return a Gaussian Mixture object.\n",
    "*  Representation of a Gaussian mixture model probability distribution.\n",
    "* #### Arguments:\n",
    " 1.    n_components: the number of mixture components (a component is a Gaussian)\n",
    " 2.    covariance_type: type of covariance parameters to use. Must be one of:\n",
    "  *                    \"full\"      : each component has its own general covariance matrix\n",
    "  *                   \"tied\"      : all components share the same general covariance matrix\n",
    "  *                   \"diag\"      : each component has its own diagonal covariance matrix\n",
    "  *                   \"spherical\" : each component has its own single variance\n",
    "  *    max_iter: the number of Expectation-Maximization iterations to perform\n",
    "\n",
    "* ####  Attributes (of the Gaussian Mixture object):\n",
    " *    weights_: weights of each mixture component.\n",
    " *    means_: mean of each mixture component\n",
    " *    covariances_: the covariance of each mixture component. \n",
    "\n",
    "* #### Methods (on the Gaussian Mixture object)\n",
    " *    fit(X) : estimate model parameters with the Expectation-Maximization for dataset X\n",
    " *    fit_predict(X): estimate model parameters and predict the labels for dataset X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE:\n",
    "# Ex.1.d.1: Gaussian Mixture vs. K-means\n",
    "\n",
    "# Data:\n",
    "X, y_true = make_blobs(n_samples=400, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "X = X[:, ::-1] # flip axes for better plotting\n",
    "rng = np.random.RandomState(13)\n",
    "X_sketched = np.dot(X, rng.randn(2, 2))\n",
    "\n",
    "\n",
    "# Computer K-means clustering on X_sketched\n",
    "### YOUR CODE HERE (Fill in the \"None\"). 2 lines of code\n",
    "# Hint: make sure that variable \"labels\" is the array of cluster indices of samples in X_ketched\n",
    "None\n",
    "None\n",
    "### END OF YOUR CODE\n",
    "\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X_sketched[:, 0], X_sketched[:, 1], c=labels, s=40, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.1.d.1: Gaussian Mixture vs. K-means (cont)\n",
    "\n",
    "# Computer Expectation-Maximization on X_sketched\n",
    "### YOUR CODE HERE (Fill in the \"None\"). 2 lines of code\n",
    "None\n",
    "None\n",
    "### END OF YOUR CODE.\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X_sketched[:, 0], X_sketched[:, 1], c=labels, s=40, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1. https://scikit-learn.org/\n",
    "2. https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/\n",
    "3. https://towardsdatascience.com/dbscan-clustering-for-data-shapes-k-means-cant-handle-well-in-python-6be89af4e6ea\n",
    "4. https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
